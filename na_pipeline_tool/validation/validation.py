"""
Medical lexicon NLP extraction pipeline

File contains: Compares the validation set with the NLP pipeline's labeling and outputs some relevant statistics afterwards.

-- (c) Rene Ahlsdorf 2019 - Team  D in the HST 953 class
"""

from na_pipeline_tool.utils import logger
from na_pipeline_tool.utils import config
from na_pipeline_tool.utils import helper_classes
from na_pipeline_tool import utils 

from na_pipeline_tool.utils import progressbar

import re
import pandas as pd
from joblib import Parallel, delayed  

import sys, os
import collections
import numpy as np 

import sklearn.metrics

class ValidationModule(helper_classes.Module):

    def __init__(self):
        super().__init__()

        self._validation_set = config.get_pipeline_config_item(self.module_name(), 'validation_set_file', None)
        self._df_notes_labeled_path = config.get_pipeline_config_item(self.module_name(), 'input_note_file', None)

        self._loaded_df = None
        self._compare_df = None
        self._orig_df = None
        self._loaded_validation = None
        self._loaded_validation_labels = None
        self._loaded_validation_label_map = None

        logger.log_info('Loading validation note labeling file')
        self._loading_validation_labeling_file()
        logger.log_info('DONE: Loading validation note labeling file')

        logger.log_info('Loading NLP pipeline processed note files')
        self._loading_note_files()
        logger.log_info('DONE: NLP pipeline processed note files')

        logger.log_info('Computing and outputting statistics')
        self._do_statistics()

    def _loading_note_files(self):
        if not self._df_notes_labeled_path:
            raise RuntimeError('Please specify a valid note input file.')
        
        def load_file(path):
            filename = utils.default_dataframe_name(path)
            assert os.path.isfile(filename), 'Could not find note parquet file: {}'.format(filename)
            df = pd.read_parquet(filename)
            df.columns = [_.upper() for _ in df.columns]

            assert 'ROW_ID' in list(df.columns), 'Notes file need to have columns: Row_id, predicted_categories'
            assert 'PREDICTED_CATEGORIES' in list(df.columns), "Processed note file needs to have the PREDICTED_CATEGORIES column generated by e.g. the negation module."
            df['PREDICTED_CATEGORIES'] = df.PREDICTED_CATEGORIES.str.upper()
            df['PREDICTED_CATEGORIES'] = df.PREDICTED_CATEGORIES.str.replace(' ', '_')
            df['PREDICTED_CATEGORIES'] = df.PREDICTED_CATEGORIES.str.split('|')
            if 'FOUND_EVIDENCE' in list(df.columns):
                df['FOUND_EVIDENCE'] = df['FOUND_EVIDENCE'].astype(bool)
                df = df[df['FOUND_EVIDENCE']]
            
            return df

        self._loaded_df = load_file(self._df_notes_labeled_path)

        unique_labels = []
        for _ in [*self._loaded_df.PREDICTED_CATEGORIES, self._loaded_validation_labels]:
            unique_labels.extend(_)

        unique_labels = set(unique_labels)
        
        lbl_id = 2
        self._loaded_validation_label_map = {"NONE" : 1}
        for _lbl in unique_labels:
            if _lbl == "NONE":
                continue
            self._loaded_validation_label_map[_lbl] = lbl_id
            lbl_id += 1

        self._loaded_df['PREDICTED_CATEGORIES'] = self._loaded_df.PREDICTED_CATEGORIES.apply(lambda x: [self._loaded_validation_label_map[_] for _ in x])
        self._loaded_validation['NOTE_TYPES'] = self._loaded_validation.NOTE_TYPES.apply(lambda x: [self._loaded_validation_label_map[_] for _ in x])

    
    def _loading_validation_labeling_file(self):
        assert self._validation_set, 'Please specify a validation labeling file.'
        try:
            with open(self._validation_set, 'r') as file:
                self._loaded_validation = file.readlines()
                self._loaded_validation = self._loaded_validation[1:]
                self._loaded_validation = [_.strip() for _ in self._loaded_validation]
                self._loaded_validation = [_.split(',') for _ in self._loaded_validation]
                self._loaded_validation = [[int(_[0]), [_.upper().replace(' ', '_') for _ in str(_[1]).split('|')], (int(_[2]) > 0)] for _ in self._loaded_validation]
                self._loaded_validation = pd.DataFrame(self._loaded_validation, columns=['ROW_ID', 'NOTE_TYPES', 'VALID_INCLUDED'])
                self._loaded_validation.loc[~self._loaded_validation['VALID_INCLUDED'], 'NOTE_TYPES'] = pd.Series([['NONE']]*self._loaded_validation.shape[0])
        except:
            raise RuntimeError('Error while processing validation labeling file. Check file structure.')
        self._loaded_validation_labels = []
        for _i, _ in self._loaded_validation.iterrows():
            self._loaded_validation_labels.extend(_['NOTE_TYPES'])

        self._loaded_validation_labels = set(self._loaded_validation_labels)


    def dump_examples_for_comparison(self):
        if not self._compare_df:
            logger.log_warn('Could not find comparison df - Skipping dumping of exemplary notes.')
            return
        
        self._get_examples_for_categories = [_.upper() for _ in self.get_examples_for_categories]
        if not self._get_examples_for_categories:
            logger.log_warn('No categories specified for dumping exemplary sentences.')
            return
        
        unknown_categories = [_ for _ in self._get_examples_for_categories if not _ in [*self._get_examples_for_categories, 'NO_FINDING']]
        if unknown_categories:
            logger.log_warn('The following categories are not present in the provided dataframes: {}'.format(unknown_categories))
            return
        
        example_list = []
        # for _cat in self._get_examples_for_categories:
        #     # Get example sentences

    def _do_statistics(self):
        validset = self._loaded_validation.sort_values('ROW_ID').reset_index(drop=True)[['ROW_ID', 'NOTE_TYPES']].copy()
        validset = validset.drop_duplicates(subset=['ROW_ID'])

        predicted = self._loaded_df[['ROW_ID', 'PREDICTED_CATEGORIES']].copy()
        predicted = predicted.rename(columns={'PREDICTED_CATEGORIES' : 'PREDICTED_CAT'})
        predicted = predicted.drop_duplicates(subset=['ROW_ID'])

        validset = validset.merge(predicted, how='left', on='ROW_ID')
        validset.loc[validset['PREDICTED_CAT'].isnull(), 'PREDICTED_CAT'] = pd.Series([[1]]*validset.shape[0])
        validset.loc[validset['NOTE_TYPES'].isnull(), 'NOTE_TYPES'] = pd.Series([[1]]*validset.shape[0])
        
        validset['MATCHED'] = validset.apply(lambda x: [_ for _ in x.NOTE_TYPES if _ in x.PREDICTED_CAT], axis=1)
        validset['UNMATCHED_VALID'] = validset.apply(lambda x: [_ for _ in x.NOTE_TYPES if _ not in x.PREDICTED_CAT], axis=1)
        validset['UNMATCHED_PREDICTED']= validset.apply(lambda x: [_ for _ in x.PREDICTED_CAT if _ not in x.NOTE_TYPES], axis=1)
        validset['CORRECT_NOTE'] = False
        
        validset.loc[(validset.UNMATCHED_VALID.str.len() == 0) & (validset.UNMATCHED_PREDICTED.str.len() == 0), 'CORRECT_NOTE'] = True
       
        max_index = max(self._loaded_validation_label_map.values())

        one_hot_valid = np.zeros((validset.shape[0], max_index))
        one_hot_pred  = np.zeros((validset.shape[0], max_index))

        _i = 0
        for _, _row in validset.iterrows():
            predicted = [_-1 for _ in _row.PREDICTED_CAT if _ != 0]
            valid = [_-1 for _ in _row.NOTE_TYPES if _ != 0]

            if valid:
                one_hot_valid[_i, valid] = 1
            if predicted:
                one_hot_pred[_i, predicted] = 1

            _i += 1
            
        # validset_types = []
        # for _, _row in validset.iterrows():
        #     unmatched_valid = sorted(_row.UNMATCHED_VALID)
        #     unmatched_predicted = sorted(_row.UNMATCHED_PREDICTED)
        #     if 0 in unmatched_valid:
        #         for _ in _row.PREDICTED_CAT:
        #            assert _ > 0
        #            _row['PREDICTED'] = _
        #            _row['VALIDATION'] = 0
        #         continue
                
        #     if 0 in unmatched_predicted:
        #         for _ in _row.NOTE_TYPES:
        #             assert _ > 0
        #             _row['PREDICTED'] = 0
        #             _row['VALIDATION'] = _
        #             validset_types.append(list(_row))
        #         continue

        #     for _nomatch in unmatched_valid:
        #         _row['PREDICTED']  = 0
        #         _row['VALIDATION'] = _nomatch
        #         validset_types.append(list(_row))

        #     for _nomatch in unmatched_predicted:
        #         _row['PREDICTED']  = _nomatch
        #         _row['VALIDATION'] = 0
        #         validset_types.append(list(_row))
        
        #     for _match in sorted(_row.MATCHED):
        #         _row['PREDICTED'] = _match
        #         _row['VALIDATION'] = _match
        #         validset_types.append(list(_row))
        
        # validset = pd.DataFrame(validset_types, columns=list(validset.columns) + ['PREDICTED', 'VALIDATION'])

        # validset['_CORRECT_ENTRIES'] = validset['PREDICTED'] == validset['VALIDATION'] 
        # validset['_CORRECT_ENTRIES'] *= 1
        # validset['_CORRECT_ENTRIES'] = validset.groupby('ROW_ID')['_CORRECT_ENTRIES'].transform(lambda x: sum(x))
        # validset['_TOTAL_ENTRIES'] = validset.groupby('ROW_ID')['_CORRECT_ENTRIES'].transform(lambda x: len(x))
        # validset['CORRECT_NOTE'] = validset['_TOTAL_ENTRIES'] == validset['_CORRECT_ENTRIES']
        # validset = validset.drop(columns=['_CORRECT_ENTRIES', '_TOTAL_ENTRIES'])

        assert len(validset.groupby('ROW_ID').first().reset_index()) == len(validset)

        logger.log_info('Correctly identified notes: {}/{} ({}%)'.format(validset['CORRECT_NOTE'].sum(), len(validset), validset['CORRECT_NOTE'].sum() * 100 / len(validset)))        

        # predicted_labels = validset['PREDICTED'].values
        # valid_labels = validset['VALIDATION'].values

        def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):
            '''
            Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case
            https://stackoverflow.com/q/32239577/395857
            '''
            acc_list = []
            for i in range(y_true.shape[0]):
                set_true = set( np.where(y_true[i])[0] )
                set_pred = set( np.where(y_pred[i])[0] )
                #print('\nset_true: {0}'.format(set_true))
                #print('set_pred: {0}'.format(set_pred))
                tmp_a = None
                if len(set_true) == 0 and len(set_pred) == 0:
                    tmp_a = 1
                else:
                    tmp_a = len(set_true.intersection(set_pred))/\
                            float( len(set_true.union(set_pred)) )
                #print('tmp_a: {0}'.format(tmp_a))
                acc_list.append(tmp_a)
            return np.mean(acc_list)
            
        logger.log_info('Hamming score: {0}'.format(hamming_score(one_hot_valid, one_hot_pred))) # 0.375 (= (0.5+1+0+0)/4)        

        # Subset accuracy
        # 0.25 (= 0+1+0+0 / 4) --> 1 if the prediction for one sample fully matches the gold. 0 otherwise.
        logger.log_info('Subset accuracy: {0}'.format(sklearn.metrics.accuracy_score(one_hot_valid, one_hot_pred, normalize=True, sample_weight=None)))

        logger.log_info('Hamming loss: {0}'.format(sklearn.metrics.hamming_loss(one_hot_valid, one_hot_pred))) 
        logger.log_info('Total instances: {0}'.format(len(one_hot_valid))) 


        logger.log_info('')
        def print_report(y_true, y_pred):
            def print_row(*args):
                logger.log_info(' '.join([str(_) for _ in args]))
            #print_row('F1 Score', sklearn.metrics.f1_score(y_true, y_pred))
            print_row('Precision', sklearn.metrics.precision_score(y_true, y_pred))
            print_row('Recall', sklearn.metrics.recall_score(y_true, y_pred))
            print_row('Accuracy', sklearn.metrics.accuracy_score(y_true, y_pred), np.sum(y_true == y_pred),'/', len(y_pred), '({}, {})'.format(y_true.sum(), y_pred.sum()))

        for _cat in range(1, max_index + 1):
            mapname = [_[0] for _ in self._loaded_validation_label_map.items() if _cat == _[1]][0]
            logger.log_info('{} =================='.format(mapname))
            print_report(one_hot_valid[:, _cat - 1],one_hot_pred[:, _cat - 1])
            logger.log_info('')
            
  
    @classmethod
    def register_argparser_object(cls, subparser_instance):
        subparser_instance.add_parser(cls.module_name(), help="Compares manually labeled notes with the framework's labeling and outputs all relevant statistics for this.")
    
